# Deepgram VAD Implementation Guide

## Overview

This guide explains how to use Deepgram's built-in Voice Activity Detection (VAD) for ultra-low latency speech detection in your voice calling platform.

---

## Why Deepgram VAD?

### Advantages over Custom VAD

**1. Zero Latency Overhead**
- VAD processing happens server-side at Deepgram
- No client-side ML model loading or processing
- Results delivered via WebSocket events

**2. Enterprise-Grade Accuracy**
- 99%+ accuracy in speech detection
- Trained on millions of hours of real-world audio
- Handles diverse accents, background noise, and edge cases

**3. Integrated with Streaming STT**
- Single WebSocket connection for both VAD and transcription
- No need to buffer audio chunks
- Real-time transcription as user speaks

**4. Production-Ready**
- Battle-tested by thousands of companies
- Built-in error handling and reconnection logic
- Scales automatically with your traffic

---

## Current Implementation (V4)

### Time-Based Detection (Current)
```typescript
// Simple 150ms silence threshold
session.silenceTimeout = setTimeout(async () => {
  await this.processUserSpeech(client, session);
}, 150);
```

**Pros:**
- Simple and reliable
- No network dependencies
- Works offline

**Cons:**
- Can't distinguish speech from noise
- False triggers from background sounds
- Fixed threshold (not adaptive)

---

## Deepgram VAD Service (Ready to Use!)

### Service Implementation

Located in `backend/src/services/deepgram.service.ts`:

```typescript
async createLiveConnectionWithVAD(options?: {
  endpointing?: number;           // Default: 200ms
  vadEvents?: boolean;            // Default: true
  language?: string;              // Default: 'en'
  onTranscript?: (result) => void;
  onSpeechStarted?: () => void;
  onSpeechEnded?: () => void;
}): Promise<LiveClient>
```

### Key Parameters

**`endpointing`** - Duration of silence to detect end of speech
- Range: 10-2000ms
- Recommended: 150-200ms for phone calls
- Lower = more responsive, higher = fewer false triggers

**`vad_events`** - Enable VAD event callbacks
- `SpeechStarted` - User began speaking
- `UtteranceEnd` - User stopped speaking (after endpointing duration)

**`interim_results`** - Get partial transcriptions
- `true` = See results as they come in (lower perceived latency)
- `false` = Only final results (simpler processing)

---

## How to Enable Deepgram VAD

### Step 1: Create Live Connection

```typescript
// In handleConnection method
const dgConnection = await deepgramService.createLiveConnectionWithVAD({
  endpointing: 150,  // 150ms silence detection
  vadEvents: true,
  language: session.config.language,

  onTranscript: (result) => {
    if (result.isFinal && result.text.trim().length > 0) {
      // Got final transcript!
      session.transcript = result.text;
    }
  },

  onSpeechStarted: () => {
    logger.info('ðŸŽ¤ User started speaking');
    session.isUserSpeaking = true;
    session.audioBuffer = [];  // Reset buffer
  },

  onSpeechEnded: async () => {
    logger.info('ðŸ”‡ User stopped speaking (Deepgram VAD)');
    session.isUserSpeaking = false;

    // Process immediately - no need to wait!
    if (!session.isProcessing && session.transcript) {
      await this.processUserSpeech(client, session);
    }
  }
});

// Store connection in session
session.deepgramConnection = dgConnection;
```

### Step 2: Stream Audio to Deepgram

```typescript
// In handleMedia method
private async handleMedia(
  client: WebSocketClient,
  session: VoiceSession,
  message: ExotelWebSocketMessage
): Promise<void> {
  if (!message.media) return;

  // Decode audio chunk
  const audioChunk = Buffer.from(message.media.payload, 'base64');

  // Send directly to Deepgram for real-time processing
  if (session.deepgramConnection) {
    session.deepgramConnection.send(audioChunk);
  } else {
    // Fallback: Buffer for batch processing
    session.audioBuffer.push(audioChunk);
  }
}
```

### Step 3: Update processUserSpeech

```typescript
private async processUserSpeech(
  client: WebSocketClient,
  session: VoiceSession
): Promise<void> {
  // Mark as processing
  session.isProcessing = true;

  try {
    let transcript: string;

    // Check if we already have transcript from streaming
    if (session.transcript) {
      transcript = session.transcript;
      session.transcript = '';  // Clear for next turn
      logger.info('Using streaming transcript from Deepgram VAD');
    } else {
      // Fallback: Batch transcription
      const audioData = Buffer.concat(session.audioBuffer);
      session.audioBuffer = [];
      transcript = await deepgramService.transcribeAudio(audioData);
    }

    // Continue with LLM and TTS...
    // ...existing code...

  } finally {
    session.isProcessing = false;
  }
}
```

### Step 4: Cleanup on Disconnect

```typescript
async handleDisconnect(client: WebSocketClient): Promise<void> {
  const session = this.sessions.get(client.id);
  if (!session) return;

  // Close Deepgram connection
  if (session.deepgramConnection) {
    try {
      session.deepgramConnection.finish();
      logger.info('Deepgram connection closed');
    } catch (error) {
      logger.error('Error closing Deepgram connection', { error });
    }
  }

  // ...existing cleanup code...
}
```

---

## Expected Performance Improvements

### Latency Comparison

**Current (Batch Processing):**
```
User stops speaking
  â†“ 150ms (time-based detection)
  â†“ 800ms (Deepgram batch STT)
  â†“ 1000ms (GPT-4o-mini)
  â†“ 200ms (Streaming TTS)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Total: ~2150ms
```

**With Deepgram VAD (Streaming):**
```
User stops speaking
  â†“ 0ms (Deepgram VAD detects instantly!)
  â†“ 500ms (Already transcribed during speech!)
  â†“ 1000ms (GPT-4o-mini)
  â†“ 200ms (Streaming TTS)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Total: ~1700ms (20% faster!)
```

### Key Benefits

1. **Instant Detection** - No artificial delay, VAD detects end of speech in real-time
2. **Streaming Transcription** - Text appears as user speaks (ready when they stop)
3. **Lower False Triggers** - ML model distinguishes speech from noise
4. **Adaptive** - Works in noisy environments, with various accents

---

## Testing Deepgram VAD

### 1. Test Connection

```bash
# In backend directory
npm run dev

# Watch for VAD events in logs
tail -f logs/combined.log | grep "Deepgram"
```

### 2. Make Test Call

Expected logs:
```
âœ… Deepgram live connection opened
ðŸŽ¤ SPEECH STARTED (Deepgram VAD)
Deepgram transcript { text: "Hello", isFinal: false }
Deepgram transcript { text: "Hello how are you", isFinal: true }
ðŸ”‡ SPEECH ENDED (Deepgram VAD)
ðŸ‘¤ USER (v4): { transcript: "Hello how are you" }
```

### 3. Compare Latency

**Measure time from user stops speaking to AI starts responding:**

Without VAD:
- Time from last audio chunk to processing start: ~150ms

With VAD:
- Time from UtteranceEnd event to processing start: ~0-50ms
- Transcript already available: saves 800ms!

---

## Configuration Tips

### For Quiet Environments
```typescript
{
  endpointing: 150,     // Aggressive (faster)
  vadEvents: true
}
```

### For Noisy Environments
```typescript
{
  endpointing: 250,     // More conservative
  vadEvents: true
}
```

### For Long-Form Speech
```typescript
{
  endpointing: 500,     // Wait longer for pauses
  vadEvents: true,
  interim_results: true // Show progress
}
```

---

## Troubleshooting

### Issue: No VAD events

**Check:**
1. `DEEPGRAM_API_KEY` is set
2. Connection opened successfully
3. Audio is being sent: `connection.send(audioChunk)`

**Debug:**
```typescript
connection.on(LiveTranscriptionEvents.Open, () => {
  console.log('Connection open!');
});

connection.on(LiveTranscriptionEvents.Error, (error) => {
  console.error('Deepgram error:', error);
});
```

### Issue: Too many false triggers

**Solution:** Increase `endpointing` duration
```typescript
endpointing: 300  // Wait 300ms instead of 150ms
```

### Issue: Slow transcription

**Check:**
1. Using `nova-2` model (fastest)
2. `interim_results: true` for progressive results
3. Audio chunks being sent continuously

---

## Cost Comparison

### Current (Batch STT)
```
$0.0043 per minute of audio (Deepgram prerecorded)
5-minute call = $0.0215
```

### With Streaming VAD
```
$0.0059 per minute of audio (Deepgram live)
5-minute call = $0.0295

Extra cost: $0.008 per call
```

**Worth it?** YES!
- 20% latency reduction
- Better UX (instant detection)
- Real-time transcription
- Only $0.008 more per call

---

## Migration Path

### Phase 1: Keep Current System (V4.0)
- Time-based detection works well
- Proven and reliable
- No changes needed

### Phase 2: Add Streaming VAD (V4.1)
- Implement Deepgram live connection
- Run in parallel with existing system
- A/B test performance

### Phase 3: Full Migration (V4.2)
- Remove time-based detection
- Use only Deepgram VAD
- Monitor for regressions

---

## Summary

Deepgram VAD is **ready to use** in your codebase:

âœ… Service implemented: `deepgram.service.ts`
âœ… VAD events supported: `SpeechStarted`, `UtteranceEnd`
âœ… Streaming transcription ready
âœ… Zero additional dependencies
âœ… Production-ready and tested

**Next Steps:**
1. Review the implementation guide above
2. Update `handleConnection` to create live connection
3. Update `handleMedia` to stream audio to Deepgram
4. Test with real calls
5. Monitor latency improvements

**Questions?** See the full implementation in:
- `backend/src/services/deepgram.service.ts`
- `backend/src/websocket/handlers/exotelVoice.handler.ts`

---

**Last Updated:** 2025-01-29
**Version:** 4.0
**Status:** ðŸŸ¢ Ready for Implementation
