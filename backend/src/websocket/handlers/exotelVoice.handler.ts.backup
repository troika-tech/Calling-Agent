import { WebSocketClient } from '../websocket.server';
import { CallLog } from '../../models/CallLog';
import { logger } from '../../utils/logger';
import { voicePipelineService, VoicePipelineConfig } from '../../services/voicePipeline.service';
import { openaiService, ChatMessage } from '../../services/openai.service';
import { anthropicService } from '../../services/anthropic.service';
import { deepgramService } from '../../services/deepgram.service';
import { deepgramTTSService } from '../../services/deepgramTTS.service';
import { audioConverter } from '../../utils/audioConverter';
import { ragService } from '../../services/rag.service';
import { buildLLMPrompt } from '../../config/systemPrompt';

/**
 * Exotel Voice WebSocket Handler
 * Handles real-time audio streaming from Exotel Voicebot applet
 *
 * ACTUAL message format from Exotel Voicebot (verified from production logs):
 * {
 *   "event": "media" | "stop",  // Note: NO "start" event in Voicebot
 *   "stream_sid": "string",     // Note: snake_case, not camelCase
 *   "sequence_number": "string",
 *   "media": {
 *     // NO "track" field in Voicebot! All media is from caller (inbound)
 *     "chunk": "string",        // audio chunk number
 *     "timestamp": "string",
 *     "payload": "string"       // base64 encoded PCM audio (16-bit, 8kHz, little-endian)
 *   },
 *   "stop": {                   // Only present when event === "stop"
 *     "call_sid": "string",
 *     "account_sid": "string",
 *     "reason": "string"
 *   }
 * }
 */

interface ExotelWebSocketMessage {
  event: 'start' | 'media' | 'stop' | 'mark';
  stream_sid?: string;        // Exotel uses snake_case
  sequence_number?: string;
  streamSid?: string;         // Keep for backward compatibility
  callSid?: string;
  mark?: {
    name: string;
  };
  media?: {
    track?: 'inbound' | 'outbound';  // Optional - not present in Voicebot
    chunk: string;
    timestamp: string;
    payload: string;           // base64 encoded PCM audio
  };
  stop?: {
    call_sid: string;
    account_sid: string;
    reason: string;
  };
}

interface VoiceSession {
  callLogId: string;
  agent: any;
  config: VoicePipelineConfig;
  audioBuffer: Buffer[];
  isProcessing: boolean;
  lastSpeechTime: number;
  firstSpeechTime?: number;  // When first audio chunk arrived (for max duration detection)
  silenceTimeout?: NodeJS.Timeout;
  streamSid?: string;  // Exotel's stream_sid for sending audio back
  sequenceNumber: number;  // Global sequence number for all audio chunks
  deepgramBuffer?: Buffer;  // Buffer for re-chunking Deepgram audio to Exotel's format
  deepgramConnection?: any;  // Deepgram live streaming connection for ultra-low latency STT
  userTranscript?: string;  // Accumulated transcript from Deepgram streaming
  partialTranscript?: string;  // Partial transcript (interim results)
  llmStarted?: boolean;  // Flag to track if LLM has started (for parallel processing)
  llmTriggeredOnPartial?: boolean;  // Track if LLM was triggered on partial (advanced optimization)
  earlyLLMResponse?: string;  // Buffer for LLM response started on partial transcript
  // Performance timing
  timings?: {
    speechStart?: number;
    speechEnd?: number;
    sttStart?: number;
    sttEnd?: number;
    ragStart?: number;
    ragEnd?: number;
    llmStart?: number;
    llmFirstToken?: number;
    llmEnd?: number;
    ttsStart?: number;
    ttsFirstChunk?: number;
    ttsEnd?: number;
    audioSendStart?: number;
    audioSendEnd?: number;
  };
}

class ExotelVoiceHandler {
  private sessions: Map<string, VoiceSession> = new Map();
  private readonly SILENCE_THRESHOLD = 150; // 150ms - ULTRA aggressive with VAD (was 200ms)
  private readonly VAD_CHECK_INTERVAL = 100; // Check VAD every 100ms for faster detection
  private readonly MAX_SPEECH_DURATION = 8000; // 8 seconds - auto-process if speaking continuously

  /**
   * Log performance metrics for a conversation turn
   */
  private logPerformanceMetrics(session: VoiceSession, stage: string): void {
    if (!session.timings) return;

    const t = session.timings;
    const metrics: { [key: string]: number } = {};

    if (t.speechStart && t.speechEnd) {
      metrics.speechDuration = t.speechEnd - t.speechStart;
    }
    if (t.sttStart && t.sttEnd) {
      metrics.sttLatency = t.sttEnd - t.sttStart;
    }
    if (t.ragStart && t.ragEnd) {
      metrics.ragLatency = t.ragEnd - t.ragStart;
    }
    if (t.llmStart && t.llmEnd) {
      metrics.llmLatency = t.llmEnd - t.llmStart;
    }
    if (t.llmStart && t.llmFirstToken) {
      metrics.llmTTFT = t.llmFirstToken - t.llmStart; // Time To First Token
    }
    if (t.ttsStart && t.ttsEnd) {
      metrics.ttsLatency = t.ttsEnd - t.ttsStart;
    }
    if (t.ttsStart && t.ttsFirstChunk) {
      metrics.ttsTTFC = t.ttsFirstChunk - t.ttsStart; // Time To First Chunk
    }
    if (t.audioSendStart && t.audioSendEnd) {
      metrics.audioSendLatency = t.audioSendEnd - t.audioSendStart;
    }
    if (t.speechEnd && t.audioSendEnd) {
      metrics.totalLatency = t.audioSendEnd - t.speechEnd; // User stops speaking ‚Üí Audio sent
    }

    logger.info(`‚è±Ô∏è PERFORMANCE [${stage}]`, metrics);
  }

  /**
   * Initialize Exotel voice session
   */
  async handleConnection(client: WebSocketClient, callLogId: string): Promise<void> {
    try {

      // Get call log and agent configuration
      const callLog = await CallLog.findById(callLogId).populate('agentId');
      if (!callLog || !callLog.agentId) {
        logger.error('Call log or agent not found', { callLogId });
        client.close(1008, 'Call log not found');
        return;
      }

      const agent = callLog.agentId as any;

      // Update call status
      callLog.status = 'in-progress';
      await callLog.save();

      const callLogObjectId = callLog._id.toString();

      const config: VoicePipelineConfig = {
        agentId: agent._id.toString(),
        callLogId: callLogObjectId,
        systemPrompt: agent.config.prompt,
        voiceProvider: agent.config.voice.provider || 'openai',
        voiceId: agent.config.voice.voiceId,
        language: agent.config.language || 'en',
        voiceSettings: {
          stability: agent.config.voice.settings?.stability ?? 0.5,
          similarityBoost: agent.config.voice.settings?.similarityBoost ?? 0.75,
          modelId: agent.config.voice.settings?.modelId
        },
        llmConfig: {
          model: agent.config.llm?.model,
          temperature: agent.config.llm?.temperature,
          maxTokens: agent.config.llm?.maxTokens
        }
      };

      await voicePipelineService.initializeSession(config, {
        existingTranscript: (callLog as any)?.transcript
      });

      // Initialize session
      const session: VoiceSession = {
        callLogId: callLogObjectId,
        agent,
        config,
        audioBuffer: [],
        isProcessing: false,
        lastSpeechTime: Date.now(),
        sequenceNumber: 0,
        userTranscript: '',
        partialTranscript: '',
        llmStarted: false,
        llmTriggeredOnPartial: false,
        earlyLLMResponse: '',
        timings: {}
      };

      this.sessions.set(client.id, session);
      client.callLogId = callLogObjectId;
      client.agentId = agent._id.toString();

      logger.info('üìû CALL STARTED', { agent: agent.name });

      // Create Deepgram live streaming connection for ultra-low latency STT
      if (deepgramService.isAvailable()) {
        try {
          const deepgramConnection = await deepgramService.createLiveConnectionWithVAD({
            endpointing: 100,
            vadEvents: true,
            language: agent.config.language || 'en',
            onTranscript: async (result) => {
              const currentSession = this.sessions.get(client.id);
              if (!currentSession) return;

              if (result.isFinal && result.text.trim().length > 0) {
                currentSession.userTranscript = (currentSession.userTranscript || '') + ' ' + result.text;
              } else if (result.text.trim().length > 0) {
                currentSession.partialTranscript = result.text;

                // Start LLM as soon as we have 3+ words (parallel processing)
                const wordCount = result.text.trim().split(/\s+/).length;
                if (!currentSession.llmStarted && !currentSession.isProcessing && wordCount >= 3) {
                  currentSession.llmStarted = true;
                  currentSession.llmTriggeredOnPartial = true;
                  if (!currentSession.timings!.llmStart) {
                    currentSession.timings!.llmStart = Date.now();
                  }

                  this.startEarlyLLMProcessing(client, currentSession, result.text).catch((error: any) => {
                    logger.error('Early LLM failed', { error: error.message });
                    currentSession.llmStarted = false;
                    currentSession.llmTriggeredOnPartial = false;
                  });
                }
              }
            },
            onSpeechEnded: async () => {
              const currentSession = this.sessions.get(client.id);
              if (!currentSession || currentSession.isProcessing) return;

              currentSession.timings!.speechEnd = Date.now();

              if (currentSession.userTranscript && currentSession.userTranscript.trim().length > 0) {
                currentSession.isProcessing = true;
                await this.processUserSpeechFromTranscript(client, currentSession);
              }
            }
          });

          session.deepgramConnection = deepgramConnection;
        } catch (error: any) {
          logger.error('Failed to create Deepgram connection', { error: error.message });
        }
      }

      // Send welcome message and first greeting
      await this.sendGreeting(client, session);

    } catch (error: any) {
      logger.error('Init failed', {
        clientId: client.id,
        callLogId,
        error: error.message
      });
      client.close(1011, 'Failed to initialize session');
    }
  }

  /**
   * Handle incoming messages from Exotel
   */
  async handleMessage(client: WebSocketClient, data: Buffer): Promise<void> {
    const session = this.sessions.get(client.id);
    if (!session) {
      // Session deleted - silently ignore (happens after disconnect)
      return;
    }

    try {
      // Parse Exotel message
      const message: ExotelWebSocketMessage = JSON.parse(data.toString());

      switch (message.event) {
        case 'start':
          await this.handleStart(client, session, message);
          break;

        case 'media':
          await this.handleMedia(client, session, message);
          break;

        case 'stop':
          await this.handleStop(client, session, message);
          break;

        case 'mark':
          await this.handleMark(client, session, message);
          break;
      }
    } catch (error: any) {
      logger.error('Error handling Exotel message', {
        clientId: client.id,
        error: error.message
      });
    }
  }

  /**
   * Handle stream start event
   */
  private async handleStart(
    client: WebSocketClient,
    session: VoiceSession,
    message: ExotelWebSocketMessage
  ): Promise<void> {
    const streamSid = message.stream_sid || message.streamSid;
    const callSid = message.callSid;

    logger.info('Exotel stream started', {
      clientId: client.id,
      streamSid,
      callSid
    });

    // Store streamSid in session for sending audio back
    session.streamSid = streamSid;

    // Update call log with stream info
    await CallLog.findByIdAndUpdate(session.callLogId, {
      $set: {
        'metadata.streamSid': streamSid,
        'metadata.exotelCallSid': callSid
      }
    });
  }

  /**
   * Handle incoming audio media from caller
   */
  private async handleMedia(
    client: WebSocketClient,
    session: VoiceSession,
    message: ExotelWebSocketMessage
  ): Promise<void> {
    if (!message.media) {
      logger.warn('Media event received but no media data', {
        clientId: client.id,
        callLogId: session.callLogId
      });
      return;
    }

    // Store streamSid from first media event (Voicebot doesn't send "start" event)
    if (!session.streamSid && (message.stream_sid || message.streamSid)) {
      session.streamSid = message.stream_sid || message.streamSid;
      logger.info('Captured stream_sid from media event', {
        clientId: client.id,
        streamSid: session.streamSid
      });
    }

    // Exotel's actual format doesn't include "track" field for Voicebot applet
    // All media events in Voicebot are bidirectional (from caller)
    // If track field exists and is "outbound", skip it (for future compatibility)
    if (message.media.track && message.media.track === 'outbound') {
      logger.debug('Ignoring outbound media track', {
        clientId: client.id,
        track: message.media.track
      });
      return;
    }

    // Decode base64 audio payload (16-bit PCM, 8kHz, mono, little-endian)
    const audioChunk = Buffer.from(message.media.payload, 'base64');

    // Send audio directly to Deepgram streaming connection for real-time transcription
    if (session.deepgramConnection) {
      try {
        session.deepgramConnection.send(audioChunk);
      } catch (error: any) {
        logger.error('Failed to send audio to Deepgram stream', {
          error: error.message
        });
        // Fall back to buffer accumulation
        session.audioBuffer.push(audioChunk);
      }
    } else {
      // No streaming connection - accumulate in buffer for batch processing
      session.audioBuffer.push(audioChunk);
    }

    const now = Date.now();
    session.lastSpeechTime = now;

    // Track when speech started (for max duration detection)
    if (!session.firstSpeechTime) {
      session.firstSpeechTime = now;
      logger.info('üé§ SPEECH START (v3)', {
        clientId: client.id
      });
    }

    // Check if we've been receiving audio for too long (user speaking continuously or background noise)
    const speechDuration = now - session.firstSpeechTime;
    if (speechDuration > this.MAX_SPEECH_DURATION && !session.isProcessing) {
      logger.info('‚è±Ô∏è MAX DURATION REACHED (v3) - auto-processing', {
        duration: `${speechDuration}ms`,
        bufferSize: session.audioBuffer.length
      });

      // Clear silence timeout
      if (session.silenceTimeout) {
        clearTimeout(session.silenceTimeout);
        session.silenceTimeout = undefined;
      }

      // Reset first speech time for next turn
      session.firstSpeechTime = undefined;

      // Mark as processing immediately
      session.isProcessing = true;

      // Process in background (no holding message)
      this.processUserSpeech(client, session).catch((error) => {
        logger.error('Error processing speech after max duration', {
          clientId: client.id,
          error: error.message
        });
      });

      return; // Don't set silence timeout
    }

    // Clear existing silence timeout
    if (session.silenceTimeout) {
      clearTimeout(session.silenceTimeout);
    }

    // Set new silence timeout (only if not already processing)
    // Note: For Deepgram VAD, use live streaming connection instead
    if (!session.isProcessing) {
      session.silenceTimeout = setTimeout(async () => {
        const silenceDetectedAt = Date.now();
        const timeSinceLastSpeech = silenceDetectedAt - session.lastSpeechTime;

        logger.info('üîî SILENCE (v4) - Deepgram ready', {
          bufferSize: session.audioBuffer.length,
          delay: `${timeSinceLastSpeech}ms`
        });

        // Reset first speech time for next turn
        session.firstSpeechTime = undefined;

        // Mark as processing
        session.isProcessing = true;

        // Process user speech directly (no holding message)
        await this.processUserSpeech(client, session);
      }, this.SILENCE_THRESHOLD);
    }
  }

  /**
   * Handle stream stop event
   */
  private async handleStop(
    client: WebSocketClient,
    session: VoiceSession,
    message: ExotelWebSocketMessage
  ): Promise<void> {
    logger.info('üõë STOP (v3)', { bufferSize: session.audioBuffer.length });

    // Process any remaining audio
    if (session.audioBuffer.length > 0 && !session.isProcessing) {
      // CRITICAL: Set isProcessing IMMEDIATELY to prevent session deletion
      session.isProcessing = true;

      logger.info('‚ö° PROCESSING (v3)', { bufferSize: session.audioBuffer.length });

      // Process in background (no holding message)
      this.processUserSpeech(client, session).catch((error) => {
        logger.error('Error processing final speech on stop', {
          clientId: client.id,
          error: error.message
        });
      });
    } else {
      logger.warn('‚ùå SKIP (v3)', {
        hasAudio: session.audioBuffer.length > 0,
        alreadyProcessing: session.isProcessing
      });
    }

    // DON'T mark call as completed here - stop just means "user stopped speaking"
    // The call is only completed when WebSocket disconnects (handled in handleDisconnect)
    logger.info('‚è∏Ô∏è STOP HANDLED (v3) - waiting for AI response');
  }

  /**
   * Handle mark event from Exotel
   * This signals that Exotel has finished processing/playing the audio we sent
   */
  private async handleMark(
    client: WebSocketClient,
    session: VoiceSession,
    message: ExotelWebSocketMessage
  ): Promise<void> {
    const markName = message.mark?.name;
    logger.info('‚úÖ MARK RECEIVED from Exotel (v13)', {
      clientId: client.id,
      markName
    });
    // Mark received means Exotel finished playing our audio
    // Session is now ready for user input automatically
  }

  /**
   * Send AI greeting to caller
   */
  private async sendGreeting(client: WebSocketClient, session: VoiceSession): Promise<void> {
    try {
      const { agent, config, callLogId } = session;
      // Use new greetingMessage field, fallback to firstMessage, then default
      const greeting = agent.config?.greetingMessage || agent.config?.firstMessage || 'Hello! How can I help you today?';

      logger.info('üé§ GENERATING GREETING (v13)', {
        greeting,
        provider: config.voiceProvider,
        voiceId: config.voiceId
      });

      let audioDurationMs = 0;

      // Use streaming for Deepgram, non-streaming for others
      if (config.voiceProvider === 'deepgram') {
        // Stream with Deepgram for ultra-low latency
        audioDurationMs = await this.streamTTSToExotel(client, greeting, session);
      } else {
        // Generate audio using TTS for other providers
        const audioBuffer = await voicePipelineService.generateFirstMessage(
          greeting,
          config
        );

        logger.info('‚úÖ GREETING AUDIO READY (v13)', {
          audioSize: audioBuffer.length
        });

        // OpenAI/ElevenLabs return MP3 - need conversion
        await this.sendAudioToExotel(client, audioBuffer, session.streamSid);
        audioDurationMs = (audioBuffer.length / 16000) * 1000;
      }

      logger.info('‚úÖ GREETING SENT (v13)');

      // Save to transcript
      await this.saveTranscript(callLogId, 'assistant', greeting);

      // Send MARK event to get notified when Exotel finishes playing
      // This is the correct way per Exotel docs - NOT "clear"!
      try {
        const markMessage = {
          event: 'mark',
          stream_sid: session.streamSid || client.id,
          mark: {
            name: `greeting_${Date.now()}`
          }
        };
        client.send(JSON.stringify(markMessage));
        logger.info('‚úÖ MARK SENT after greeting (v13) - will wait for mark event from Exotel');
      } catch (error: any) {
        logger.warn('Failed to send mark message after greeting', {
          clientId: client.id,
          error: error.message
        });
      }

    } catch (error: any) {
      logger.error('‚ùå GREETING FAILED (v13)', {
        clientId: client.id,
        error: error.message,
        stack: error.stack
      });
    }
  }

  /**
   * Start early LLM processing based on partial transcript (PARALLEL PROCESSING)
   * This is called as soon as we have 3+ words, while user is still speaking!
   * Ultra-low latency optimization - LLM starts before user finishes speaking
   */
  private async startEarlyLLMProcessing(
    client: WebSocketClient,
    session: VoiceSession,
    partialTranscript: string
  ): Promise<void> {
    try {
      logger.info('üöÄ EARLY LLM PROCESSING (v5)', {
        partial: partialTranscript,
        wordCount: partialTranscript.split(/\s+/).length
      });

      // Get conversation history
      const conversationHistory = await this.getConversationHistoryMessages(session.callLogId);

      // Get agent persona
      const agentPersona = session.agent.config.persona || session.agent.config.prompt;

      // For early LLM, we skip RAG to reduce latency (RAG will be done on final transcript if needed)
      // This is a trade-off: faster response vs less context
      // You can enable RAG here if needed, but it adds ~200-500ms

      // Build system prompt (without RAG for speed)
      const systemPrompt = buildLLMPrompt({
        agentPersona,
        ragContext: undefined  // Skip RAG for early LLM to maximize speed
      });

      // Prepare messages with PARTIAL transcript
      const messages: ChatMessage[] = [
        { role: 'system', content: systemPrompt },
        ...conversationHistory,
        { role: 'user', content: partialTranscript }  // Using partial transcript!
      ];

      // Determine which LLM service to use
      const model = session.agent.config?.llm?.model || 'gpt-4o-mini';
      const isClaude = model.startsWith('claude-');

      // Get streaming generator
      const streamGenerator = isClaude && anthropicService.isAvailable()
        ? anthropicService.getChatCompletionStream(
            messages as any,
            {
              model,
              temperature: session.agent.config?.llm?.temperature || 0.7,
              maxTokens: session.agent.config?.llm?.maxTokens,
              systemPrompt
            }
          )
        : openaiService.getChatCompletionStream(messages, {
            model,
            temperature: session.agent.config?.llm?.temperature || 0.7,
            maxTokens: session.agent.config?.llm?.maxTokens
          });

      let earlyResponse = '';
      let sentenceBuffer = '';
      const sentenceEnders = ['.', '!', '?', '\n'];

      logger.info('‚ö° LLM streaming started (while user still speaking)');

      // Stream the LLM response
      for await (const chunk of streamGenerator) {
        earlyResponse += chunk;
        sentenceBuffer += chunk;

        // Store early response in session
        session.earlyLLMResponse = earlyResponse;

        // Check if we have a complete sentence
        const lastChar = sentenceBuffer.trim().slice(-1);
        if (sentenceEnders.includes(lastChar) && sentenceBuffer.trim().length > 10) {
          const sentence = sentenceBuffer.trim();

          logger.info('‚ö° Early LLM sentence ready', { sentence });

          // Synthesize and stream sentence immediately
          if (session.config.voiceProvider === 'deepgram') {
            await this.streamTTSToExotel(client, sentence, session);
          } else {
            const audioResponse = await voicePipelineService.synthesizeText(
              sentence,
              session.config
            );
            await this.sendAudioToExotel(client, audioResponse, session.streamSid);
          }

          // Clear sentence buffer
          sentenceBuffer = '';
        }
      }

      // Send any remaining text in buffer
      if (sentenceBuffer.trim().length > 0) {
        if (session.config.voiceProvider === 'deepgram') {
          await this.streamTTSToExotel(client, sentenceBuffer.trim(), session);
        } else {
          const audioResponse = await voicePipelineService.synthesizeText(
            sentenceBuffer.trim(),
            session.config
          );
          await this.sendAudioToExotel(client, audioResponse, session.streamSid);
        }
      }

      logger.info('‚úÖ Early LLM complete', {
        response: earlyResponse,
        triggeredOnPartial: true
      });

      // Save to transcript (this is the AI's response to the partial transcript)
      await this.saveTranscript(session.callLogId, 'assistant', earlyResponse);

      // Send MARK event
      try {
        const markMessage = {
          event: 'mark',
          stream_sid: session.streamSid || client.id,
          mark: {
            name: `early_response_${Date.now()}`
          }
        };
        client.send(JSON.stringify(markMessage));
      } catch (error: any) {
        logger.warn('Failed to send mark message', { error: error.message });
      }

    } catch (error: any) {
      logger.error('Error in early LLM processing', {
        error: error.message,
        stack: error.stack
      });
      throw error;
    } finally {
      // Reset flags when early processing completes
      session.llmStarted = false;
      session.isProcessing = false;
    }
  }

  /**
   * Process user speech from streaming transcript (Deepgram VAD)
   * This is the ULTRA-LOW LATENCY path - no STT needed!
   */
  private async processUserSpeechFromTranscript(
    client: WebSocketClient,
    session: VoiceSession
  ): Promise<void> {
    logger.info('‚ö° PROCESS FROM TRANSCRIPT (v5 - Parallel)', {
      transcript: session.userTranscript,
      earlyLLMTriggered: session.llmTriggeredOnPartial
    });

    try {
      const transcript = (session.userTranscript || '').trim();

      if (!transcript || transcript.length === 0) {
        logger.warn('‚ùå PROCESS ABORT (v5) - no transcript');
        session.isProcessing = false;
        return;
      }

      // Check if early LLM was already triggered on partial transcript
      if (session.llmTriggeredOnPartial && session.earlyLLMResponse) {
        logger.info('‚úÖ Early LLM already processed (v5)', {
          earlyResponse: session.earlyLLMResponse,
          finalTranscript: transcript
        });

        // Early LLM already handled this - just update transcript and reset flags
        await this.saveTranscript(session.callLogId, 'user', transcript);

        // Reset flags for next turn
        session.userTranscript = '';
        session.partialTranscript = '';
        session.llmTriggeredOnPartial = false;
        session.earlyLLMResponse = '';
        session.isProcessing = false;

        logger.info('‚ö° PARALLEL PROCESSING COMPLETE - Response already sent!');
        return;
      }

      // Clear transcript for next turn
      session.userTranscript = '';
      session.partialTranscript = '';

      logger.info('üë§ USER (v4 - Streaming):', { transcript });

      // Save user transcript
      await this.saveTranscript(session.callLogId, 'user', transcript);

      // Check for end call phrases
      if (this.shouldEndCall(transcript, session.agent.config.endCallPhrases)) {
        logger.info('üîö END CALL PHRASE DETECTED', {
          transcript,
          matchedPhrases: session.agent.config.endCallPhrases
        });

        // Send polite goodbye and end call
        const goodbyeMessage = 'Thank you for calling! Have a great day. Goodbye!';
        await this.sendFinalResponse(client, goodbyeMessage, session);
        await this.saveTranscript(session.callLogId, 'assistant', goodbyeMessage);

        // Close the call
        session.isProcessing = false;
        client.close(1000, 'Call ended by user');
        return;
      }

      // Get conversation history and prepare for LLM
      const conversationHistory = await this.getConversationHistoryMessages(session.callLogId);

      // Get agent persona (prefer new 'persona' field, fallback to 'prompt' for backward compatibility)
      const agentPersona = session.agent.config.persona || session.agent.config.prompt;

      logger.info('ü§ñ Building LLM prompt', {
        hasPersona: !!agentPersona,
        personaLength: agentPersona?.length
      });

      // RAG: Query knowledge base if query is relevant
      let ragContextFormatted: string | undefined;

      if (ragService.isQueryRelevantForKB(transcript)) {
        try {
          logger.info('üîç RAG: Query is relevant, searching knowledge base');

          const ragContext = await ragService.queryKnowledgeBase(
            transcript,
            session.agent._id.toString(),
            {
              topK: 3,  // Limit to 3 chunks for phone conversations (keep context short)
              minScore: 0.7,
              maxContextLength: 2000  // ~500 tokens max for phone context
            }
          );

          if (ragContext.chunks.length > 0) {
            logger.info('‚úÖ RAG: Found relevant context', {
              chunks: ragContext.totalChunks,
              maxScore: ragContext.maxScore.toFixed(3),
              avgScore: ragContext.avgScore.toFixed(3)
            });

            // Format RAG context for LLM
            ragContextFormatted = ragService.formatContextForLLM(ragContext);
          } else {
            logger.info('‚ö†Ô∏è RAG: No relevant context found');
          }
        } catch (error: any) {
          logger.error('‚ùå RAG: Failed to query knowledge base', {
            error: error.message
          });
          // Continue without RAG context - don't fail the entire call
        }
      } else {
        logger.debug('RAG: Query not relevant for KB (conversational/greeting)');
      }

      // Build complete system prompt: Global Rules + Agent Persona + RAG Context
      const systemPrompt = buildLLMPrompt({
        agentPersona,
        ragContext: ragContextFormatted
      });

      logger.debug('System prompt built', {
        totalLength: systemPrompt.length,
        hasRAG: !!ragContextFormatted
      });

      // ALWAYS include system prompt at the start (it's not in conversationHistory)
      // Format: System Prompt + Chat History + Current User Message
      const messages: ChatMessage[] = [
        { role: 'system', content: systemPrompt },
        ...conversationHistory,
        { role: 'user', content: transcript }
      ];

      // Generate AI response using LLM with streaming

      let fullResponse = '';
      let sentenceBuffer = '';
      const sentenceEnders = ['.', '!', '?', '\n'];

      // Determine which LLM service to use based on model
      const model = session.agent.config?.llm?.model || 'gpt-4o-mini';
      const isClaude = model.startsWith('claude-');

      // Get streaming generator based on model type
      const streamGenerator = isClaude && anthropicService.isAvailable()
        ? anthropicService.getChatCompletionStream(
            messages as any,
            {
              model,
              temperature: session.agent.config?.llm?.temperature || 0.7,
              maxTokens: session.agent.config?.llm?.maxTokens,
              systemPrompt  // Use the built system prompt for Claude
            }
          )
        : openaiService.getChatCompletionStream(messages, {
            model,
            temperature: session.agent.config?.llm?.temperature || 0.7,
            maxTokens: session.agent.config?.llm?.maxTokens
          });

      // Track total audio duration to wait before sending "clear"
      let totalAudioDurationMs = 0;

      // Stream the LLM response and process sentence-by-sentence
      for await (const chunk of streamGenerator) {
        fullResponse += chunk;
        sentenceBuffer += chunk;

        // Check if we have a complete sentence
        const lastChar = sentenceBuffer.trim().slice(-1);
        if (sentenceEnders.includes(lastChar) && sentenceBuffer.trim().length > 10) {
          const sentence = sentenceBuffer.trim();

          // Synthesize and stream sentence with ULTRA-LOW LATENCY
          // Deepgram streaming TTS: Send audio chunks as they're generated (sub-200ms TTFB!)
          if (session.config.voiceProvider === 'deepgram') {
            const audioDurationMs = await this.streamTTSToExotel(client, sentence, session);
            totalAudioDurationMs += audioDurationMs;
          } else {
            // Non-streaming fallback for OpenAI/ElevenLabs
            const audioResponse = await voicePipelineService.synthesizeText(
              sentence,
              session.config
            );
            await this.sendAudioToExotel(client, audioResponse, session.streamSid);
            totalAudioDurationMs += (audioResponse.length / 16000) * 1000;
          }

          // Clear sentence buffer
          sentenceBuffer = '';
        }
      }

      // Send any remaining text in buffer
      if (sentenceBuffer.trim().length > 0) {
        // Use streaming TTS for remaining buffer too
        if (session.config.voiceProvider === 'deepgram') {
          const audioDurationMs = await this.streamTTSToExotel(client, sentenceBuffer.trim(), session);
          totalAudioDurationMs += audioDurationMs;
        } else {
          const audioResponse = await voicePipelineService.synthesizeText(
            sentenceBuffer.trim(),
            session.config
          );
          await this.sendAudioToExotel(client, audioResponse, session.streamSid);
          totalAudioDurationMs += (audioResponse.length / 16000) * 1000;
        }
      }

      logger.info('ü§ñ AI (v4 - Streaming):', { response: fullResponse });

      // Save complete AI response to transcript
      await this.saveTranscript(session.callLogId, 'assistant', fullResponse);

      // Send MARK event to get notified when Exotel finishes playing
      try {
        const markMessage = {
          event: 'mark',
          stream_sid: session.streamSid || client.id,
          mark: {
            name: `response_${Date.now()}`
          }
        };
        client.send(JSON.stringify(markMessage));
        logger.info('‚úÖ MARK SENT after response (v4) - will wait for mark event from Exotel');
      } catch (error: any) {
        logger.warn('Failed to send mark message', {
          clientId: client.id,
          error: error.message
        });
      }

    } catch (error: any) {
      logger.error('Error processing user speech from transcript', {
        clientId: client.id,
        error: error.message
      });
    } finally {
      session.isProcessing = false;
    }
  }

  /**
   * Process accumulated user speech (FALLBACK for batch STT)
   */
  private async processUserSpeech(
    client: WebSocketClient,
    session: VoiceSession
  ): Promise<void> {
    logger.info('üé§ PROCESS START (v3)', {
      hasBuffer: session.audioBuffer.length > 0
    });

    // Don't check isProcessing here - caller already checked and set it
    if (session.audioBuffer.length === 0) {
      logger.warn('‚ùå PROCESS ABORT (v3) - no audio');
      session.isProcessing = false;
      return;
    }

    try {
      // Combine audio chunks
      const audioData = Buffer.concat(session.audioBuffer);
      session.audioBuffer = [];

      logger.info('üéôÔ∏è TRANSCRIBING (v3)', { audioSize: audioData.length });

      // Convert incoming audio to PCM for transcription
      const pcmAudio = await this.convertIncomingAudioToPCM(audioData);

      // Transcribe with Deepgram (much faster than Whisper: <1s vs 8s)
      // Falls back to Whisper if Deepgram not available or returns empty
      let transcript: string;

      if (deepgramService.isAvailable()) {
        logger.info('Using Deepgram for fast transcription');
        transcript = await deepgramService.transcribeAudio(pcmAudio);

        // If Deepgram returns empty, try Whisper as fallback
        if (!transcript || transcript.trim().length === 0) {
          logger.warn('‚ö†Ô∏è Deepgram returned empty transcript, falling back to Whisper');
          const transcription = await openaiService.transcribeAudio(
            pcmAudio,
            session.agent.config?.language || 'en'
          );
          transcript = transcription.text;
          logger.info('‚úÖ Whisper fallback result', {
            transcript: transcript || '(empty)',
            length: transcript?.length || 0
          });
        }
      } else {
        logger.info('Deepgram not available, falling back to Whisper');
        const transcription = await openaiService.transcribeAudio(
          pcmAudio,
          session.agent.config?.language || 'en'
        );
        transcript = transcription.text;
      }

      if (!transcript || transcript.trim().length === 0) {
        logger.warn('‚ö†Ô∏è No speech detected in audio (both Deepgram and Whisper returned empty)');
        session.isProcessing = false;
        return;
      }

      logger.info('üë§ USER (v3):', { transcript });

      // Save user transcript
      await this.saveTranscript(session.callLogId, 'user', transcript);

      // Check for end call phrases
      if (this.shouldEndCall(transcript, session.agent.config.endCallPhrases)) {
        logger.info('üîö END CALL PHRASE DETECTED', {
          transcript,
          matchedPhrases: session.agent.config.endCallPhrases
        });

        // Send polite goodbye and end call
        const goodbyeMessage = 'Thank you for calling! Have a great day. Goodbye!';
        await this.sendFinalResponse(client, goodbyeMessage, session);
        await this.saveTranscript(session.callLogId, 'assistant', goodbyeMessage);

        // Close the call
        session.isProcessing = false;
        client.close(1000, 'Call ended by user');
        return;
      }

      // Get conversation history and prepare for LLM
      const conversationHistory = await this.getConversationHistoryMessages(session.callLogId);

      // Get agent persona (prefer new 'persona' field, fallback to 'prompt' for backward compatibility)
      const agentPersona = session.agent.config.persona || session.agent.config.prompt;

      logger.info('ü§ñ Building LLM prompt', {
        hasPersona: !!agentPersona,
        personaLength: agentPersona?.length
      });

      // RAG: Query knowledge base if query is relevant
      let ragContextFormatted: string | undefined;

      if (ragService.isQueryRelevantForKB(transcript)) {
        try {
          logger.info('üîç RAG: Query is relevant, searching knowledge base');

          const ragContext = await ragService.queryKnowledgeBase(
            transcript,
            session.agent._id.toString(),
            {
              topK: 3,  // Limit to 3 chunks for phone conversations (keep context short)
              minScore: 0.7,
              maxContextLength: 2000  // ~500 tokens max for phone context
            }
          );

          if (ragContext.chunks.length > 0) {
            logger.info('‚úÖ RAG: Found relevant context', {
              chunks: ragContext.totalChunks,
              maxScore: ragContext.maxScore.toFixed(3),
              avgScore: ragContext.avgScore.toFixed(3)
            });

            // Format RAG context for LLM
            ragContextFormatted = ragService.formatContextForLLM(ragContext);
          } else {
            logger.info('‚ö†Ô∏è RAG: No relevant context found');
          }
        } catch (error: any) {
          logger.error('‚ùå RAG: Failed to query knowledge base', {
            error: error.message
          });
          // Continue without RAG context - don't fail the entire call
        }
      } else {
        logger.debug('RAG: Query not relevant for KB (conversational/greeting)');
      }

      // Build complete system prompt: Global Rules + Agent Persona + RAG Context
      const systemPrompt = buildLLMPrompt({
        agentPersona,
        ragContext: ragContextFormatted
      });

      logger.debug('System prompt built', {
        totalLength: systemPrompt.length,
        hasRAG: !!ragContextFormatted
      });

      // ALWAYS include system prompt at the start (it's not in conversationHistory)
      // Format: System Prompt + Chat History + Current User Message
      const messages: ChatMessage[] = [
        { role: 'system', content: systemPrompt },
        ...conversationHistory,
        { role: 'user', content: transcript }
      ];

      // Generate AI response using LLM with streaming

      let fullResponse = '';
      let sentenceBuffer = '';
      const sentenceEnders = ['.', '!', '?', '\n'];

      // Determine which LLM service to use based on model
      const model = session.agent.config?.llm?.model || 'gpt-4o-mini';
      const isClaude = model.startsWith('claude-');

      // Get streaming generator based on model type
      const streamGenerator = isClaude && anthropicService.isAvailable()
        ? anthropicService.getChatCompletionStream(
            messages as any,
            {
              model,
              temperature: session.agent.config?.llm?.temperature || 0.7,
              maxTokens: session.agent.config?.llm?.maxTokens,
              systemPrompt  // Use the built system prompt for Claude
            }
          )
        : openaiService.getChatCompletionStream(messages, {
            model,
            temperature: session.agent.config?.llm?.temperature || 0.7,
            maxTokens: session.agent.config?.llm?.maxTokens
          });

      // Track total audio duration to wait before sending "clear"
      let totalAudioDurationMs = 0;

      // Stream the LLM response and process sentence-by-sentence
      for await (const chunk of streamGenerator) {
        fullResponse += chunk;
        sentenceBuffer += chunk;

        // Check if we have a complete sentence
        const lastChar = sentenceBuffer.trim().slice(-1);
        if (sentenceEnders.includes(lastChar) && sentenceBuffer.trim().length > 10) {
          const sentence = sentenceBuffer.trim();

          // Synthesize and stream sentence with ULTRA-LOW LATENCY
          // Deepgram streaming TTS: Send audio chunks as they're generated (sub-200ms TTFB!)
          if (session.config.voiceProvider === 'deepgram') {
            const audioDurationMs = await this.streamTTSToExotel(client, sentence, session);
            totalAudioDurationMs += audioDurationMs;
          } else {
            // Non-streaming fallback for OpenAI/ElevenLabs
            const audioResponse = await voicePipelineService.synthesizeText(
              sentence,
              session.config
            );
            await this.sendAudioToExotel(client, audioResponse, session.streamSid);
            totalAudioDurationMs += (audioResponse.length / 16000) * 1000;
          }

          // Clear sentence buffer
          sentenceBuffer = '';
        }
      }

      // Send any remaining text in buffer
      if (sentenceBuffer.trim().length > 0) {
        // Use streaming TTS for remaining buffer too
        if (session.config.voiceProvider === 'deepgram') {
          const audioDurationMs = await this.streamTTSToExotel(client, sentenceBuffer.trim(), session);
          totalAudioDurationMs += audioDurationMs;
        } else {
          const audioResponse = await voicePipelineService.synthesizeText(
            sentenceBuffer.trim(),
            session.config
          );
          await this.sendAudioToExotel(client, audioResponse, session.streamSid);
          totalAudioDurationMs += (audioResponse.length / 16000) * 1000;
        }
      }

      logger.info('ü§ñ AI (v13):', { response: fullResponse });

      // Save complete AI response to transcript
      await this.saveTranscript(session.callLogId, 'assistant', fullResponse);

      // Send MARK event to get notified when Exotel finishes playing
      // This is the correct way per Exotel docs - NOT "clear"!
      try {
        const markMessage = {
          event: 'mark',
          stream_sid: session.streamSid || client.id,
          mark: {
            name: `response_${Date.now()}`
          }
        };
        client.send(JSON.stringify(markMessage));
        logger.info('‚úÖ MARK SENT after response (v13) - will wait for mark event from Exotel');
      } catch (error: any) {
        logger.warn('Failed to send mark message', {
          clientId: client.id,
          error: error.message
        });
      }

    } catch (error: any) {
      logger.error('Error processing user speech', {
        clientId: client.id,
        error: error.message
      });
    } finally {
      session.isProcessing = false;
    }
  }

  /**
   * Send already-converted PCM audio to Exotel (no conversion needed)
   * Used for pre-cached holding messages
   */
  private async sendPCMAudioToExotel(
    client: WebSocketClient,
    pcmAudio: Buffer,
    streamSid?: string
  ): Promise<void> {
    try {
      // Get session to access global sequence number
      const session = this.sessions.get(client.id);
      if (!session) {
        logger.error('No session found when sending PCM audio', {
          clientId: client.id
        });
        return;
      }

      // Exotel requires chunks in multiples of 320 bytes
      // Minimum: 3.2k (100ms), Maximum: 100k
      // We'll use 3200 bytes (100ms of audio at 8kHz 16-bit mono)
      const chunkSize = 3200; // 100ms chunks

      // Check if WebSocket is still connected
      if (client.readyState !== 1) { // 1 = OPEN
        logger.error('WebSocket not open when trying to send audio', {
          clientId: client.id,
          readyState: client.readyState,
          states: { CONNECTING: 0, OPEN: 1, CLOSING: 2, CLOSED: 3 }
        });
        return;
      }

      const startSequence = session.sequenceNumber;

      // Streaming audio to caller

      for (let i = 0; i < pcmAudio.length; i += chunkSize) {
        // Check WebSocket state before EACH chunk
        if (client.readyState !== 1) {
          logger.warn('WebSocket closed mid-stream, stopping audio transmission', {
            clientId: client.id,
            chunksSent: session.sequenceNumber - startSequence,
            totalChunks: Math.ceil(pcmAudio.length / chunkSize)
          });
          break;
        }

        const chunk = pcmAudio.slice(i, i + chunkSize);
        const payload = chunk.toString('base64');

        const message = {
          event: 'media',
          stream_sid: streamSid || client.id,
          sequence_number: session.sequenceNumber.toString(),
          media: {
            track: 'outbound',  // Explicitly mark as outbound to caller
            chunk: session.sequenceNumber.toString(),
            timestamp: Date.now().toString(),
            payload
          }
        };

        client.send(JSON.stringify(message));
        session.sequenceNumber++;  // Increment global sequence number

        // ‚ö° v6 OPTIMIZATION: Removed 20ms delay for ultra-low latency
        // Modern WebSockets and Exotel can handle bursts without artificial throttling
        // Saves ~20ms √ó chunks = ~1000ms for typical 5-second audio!
        // Network backpressure is handled automatically by WebSocket protocol
      }

      // Audio sent successfully

    } catch (error: any) {
      logger.error('Error sending PCM audio to Exotel', {
        clientId: client.id,
        error: error.message
      });
    }
  }

  /**
   * Send audio to Exotel (convert MP3/WAV to PCM first, then stream)
   * Used for AI-generated responses that come as MP3
   */
  private async sendAudioToExotel(
    client: WebSocketClient,
    audioBuffer: Buffer,
    streamSid?: string
  ): Promise<void> {
    try {
      // Convert MP3/WAV to Linear PCM format (16-bit, 8kHz, mono, little-endian)
      const pcmAudio = await this.convertToPCM(audioBuffer);

      // Use the PCM sender
      await this.sendPCMAudioToExotel(client, pcmAudio, streamSid);
    } catch (error: any) {
      logger.error('Error converting and sending audio to Exotel', {
        clientId: client.id,
        error: error.message
      });
    }
  }

  /**
   * Stream TTS directly to Exotel with ULTRA-LOW latency (Deepgram only)
   * Sends audio chunks as they're generated - sub-200ms TTFB!
   * This is the fastest possible method for voice responses.
   */
  private async streamTTSToExotel(
    client: WebSocketClient,
    text: string,
    session: VoiceSession
  ): Promise<number> {
    try {
      logger.info('üé§ STREAMING TTS (v7)', { textLength: text.length });

      // CRITICAL: Clear buffer at start to prevent corruption from previous sentence
      session.deepgramBuffer = Buffer.alloc(0);

      let totalAudioBytes = 0;
      let chunkCount = 0;

      // Use Deepgram's streaming TTS with callback - PROCESS IMMEDIATELY for low latency
      await deepgramTTSService.synthesizeStreaming(
        text,
        async (audioChunk: Buffer) => {
          // Process chunk immediately as it arrives (true streaming!)
          await this.sendDeepgramChunkToExotel(client, audioChunk, session);
          totalAudioBytes += audioChunk.length;
          chunkCount++;
        },
        session.config.voiceId || 'aura-asteria-en'
      );

      // CRITICAL: Flush any remaining audio in buffer after all chunks processed
      const flushedBytes = await this.flushDeepgramBuffer(client, session);
      totalAudioBytes += flushedBytes;

      // Calculate audio duration: bytes / (sample_rate * bytes_per_sample * channels)
      // 8000 Hz * 2 bytes (16-bit) * 1 channel = 16000 bytes/second
      const audioDurationMs = (totalAudioBytes / 16000) * 1000;

      logger.info('‚úÖ STREAMING TTS COMPLETE (v7)', {
        totalChunks: chunkCount,
        totalBytes: totalAudioBytes,
        durationMs: Math.round(audioDurationMs)
      });

      return audioDurationMs;
    } catch (error: any) {
      logger.error('Failed to stream TTS to Exotel', {
        clientId: client.id,
        error: error.message
      });
      // Fallback to non-streaming
      const audioBuffer = await voicePipelineService.synthesizeText(text, session.config);
      await this.sendPCMAudioToExotel(client, audioBuffer, session.streamSid);
      // Return duration for fallback audio
      return (audioBuffer.length / 16000) * 1000;
    }
  }

  /**
   * Send Deepgram audio chunk to Exotel with proper chunking
   * CRITICAL: Exotel expects 3200-byte chunks (100ms frames)
   * Deepgram sends variable-sized chunks, so we buffer and re-chunk
   */
  private async sendDeepgramChunkToExotel(
    client: WebSocketClient,
    audioChunk: Buffer,
    session: VoiceSession
  ): Promise<void> {
    // Check WebSocket is still connected
    if (client.readyState !== 1) {
      logger.warn('WebSocket not open, skipping chunk', {
        clientId: client.id,
        readyState: client.readyState
      });
      return;
    }

    // Initialize buffer if not exists
    if (!session.deepgramBuffer) {
      session.deepgramBuffer = Buffer.alloc(0);
    }

    // Append new chunk to buffer
    session.deepgramBuffer = Buffer.concat([session.deepgramBuffer, audioChunk]);

    // Send in 3200-byte chunks (100ms frames @ 8kHz 16-bit mono)
    const CHUNK_SIZE = 3200;
    while (session.deepgramBuffer.length >= CHUNK_SIZE) {
      const chunk = session.deepgramBuffer.slice(0, CHUNK_SIZE);
      session.deepgramBuffer = session.deepgramBuffer.slice(CHUNK_SIZE);

      const payload = chunk.toString('base64');

      const message = {
        event: 'media',
        stream_sid: session.streamSid || client.id,
        sequence_number: session.sequenceNumber.toString(),
        media: {
          track: 'outbound',
          chunk: session.sequenceNumber.toString(),
          timestamp: Date.now().toString(),
          payload
        }
      };

      client.send(JSON.stringify(message));
      session.sequenceNumber++;

      // ‚ö° v6 OPTIMIZATION: Removed 20ms delay for ultra-low latency streaming
      // WebSocket handles flow control automatically with TCP backpressure
      // Saves ~1000ms for typical streaming TTS responses!
    }
  }

  /**
   * Flush any remaining audio in Deepgram buffer (partial chunk < 3200 bytes)
   * CRITICAL: Call this after each TTS stream completes to send final audio
   * Returns the number of bytes flushed (excluding padding)
   */
  private async flushDeepgramBuffer(
    client: WebSocketClient,
    session: VoiceSession
  ): Promise<number> {
    if (!session.deepgramBuffer || session.deepgramBuffer.length === 0) {
      return 0;
    }

    // Check WebSocket is still connected
    if (client.readyState !== 1) {
      logger.warn('WebSocket not open, cannot flush buffer', {
        clientId: client.id
      });
      return 0;
    }

    // Send remaining audio (pad to 320-byte multiple if needed)
    const remainingAudio = session.deepgramBuffer;
    const paddingNeeded = (320 - (remainingAudio.length % 320)) % 320;
    const paddedAudio = paddingNeeded > 0
      ? Buffer.concat([remainingAudio, Buffer.alloc(paddingNeeded)])
      : remainingAudio;

    const payload = paddedAudio.toString('base64');

    const message = {
      event: 'media',
      stream_sid: session.streamSid || client.id,
      sequence_number: session.sequenceNumber.toString(),
      media: {
        track: 'outbound',
        chunk: session.sequenceNumber.toString(),
        timestamp: Date.now().toString(),
        payload
      }
    };

    client.send(JSON.stringify(message));
    session.sequenceNumber++;

    // Clear buffer
    session.deepgramBuffer = Buffer.alloc(0);

    logger.debug('Flushed remaining audio', {
      size: remainingAudio.length,
      padded: paddingNeeded
    });

    return remainingAudio.length;
  }

  /**
   * Convert Exotel PCM audio to WAV for Whisper
   */
  private async convertIncomingAudioToPCM(audioData: Buffer): Promise<Buffer> {
    try {
      // Exotel Voicebot sends raw 16-bit 8kHz mono PCM (little-endian)
      // Whisper needs 16kHz WAV format
      return await audioConverter.convertExotelPCMToWAV(audioData);
    } catch (error: any) {
      logger.error('Failed to convert incoming audio to WAV', {
        error: error.message
      });
      // Return original buffer if conversion fails
      return audioData;
    }
  }

  /**
   * Convert TTS output to PCM for Exotel
   */
  private async convertToPCM(audioData: Buffer): Promise<Buffer> {
    try {
      // Convert MP3/WAV from TTS to 16-bit 8kHz PCM for Exotel
      return await audioConverter.convertToPCM(audioData);
    } catch (error: any) {
      logger.error('Failed to convert TTS audio to PCM', {
        error: error.message
      });
      // Return original buffer if conversion fails
      return audioData;
    }
  }

  /**
   * Save message to call transcript
   */
  private async saveTranscript(
    callLogId: string,
    speaker: 'user' | 'assistant',
    text: string
  ): Promise<void> {
    try {
      await CallLog.findByIdAndUpdate(callLogId, {
        $push: {
          transcript: {
            speaker,
            text,
            timestamp: new Date()
          }
        }
      });
    } catch (error: any) {
      logger.error('Error saving transcript', {
        callLogId,
        error: error.message
      });
    }
  }

  /**
   * Get conversation history for context
   */
  private async getConversationHistory(callLogId: string): Promise<string> {
    try {
      const callLog = await CallLog.findById(callLogId);
      if (!callLog || !callLog.transcript) {
        return '';
      }

      return callLog.transcript
        .map((t: any) => `${t.speaker}: ${t.text}`)
        .join('\n');
    } catch (error: any) {
      logger.error('Error getting conversation history', {
        callLogId,
        error: error.message
      });
      return '';
    }
  }

  /**
   * Get conversation history as ChatMessage array for LLM
   */
  private async getConversationHistoryMessages(callLogId: string): Promise<ChatMessage[]> {
    try {
      const callLog = await CallLog.findById(callLogId);
      if (!callLog || !callLog.transcript) {
        return [];
      }

      return callLog.transcript
        .map((t: any) => ({
          role: (
            t.speaker === 'assistant' || t.speaker === 'agent' ? 'assistant' : 'user'
          ) as 'user' | 'assistant' | 'system',
          content: t.text
        }));
    } catch (error: any) {
      logger.error('Error getting conversation history messages', {
        callLogId,
        error: error.message
      });
      return [];
    }
  }

  /**
   * Check if transcript contains end call phrases
   */
  private shouldEndCall(transcript: string, endCallPhrases: string[] = []): boolean {
    if (!endCallPhrases || endCallPhrases.length === 0) {
      return false;
    }

    const lowerTranscript = transcript.toLowerCase().trim();

    // Check for exact matches or partial matches
    for (const phrase of endCallPhrases) {
      const lowerPhrase = phrase.toLowerCase().trim();

      // Exact match
      if (lowerTranscript === lowerPhrase) {
        return true;
      }

      // Phrase at end of transcript (common for goodbyes)
      if (lowerTranscript.endsWith(lowerPhrase)) {
        return true;
      }

      // Phrase as standalone word/phrase (surrounded by spaces or punctuation)
      const regex = new RegExp(`(^|\\s|[.,!?])${lowerPhrase.replace(/[.*+?^${}()|[\]\\]/g, '\\$&')}($|\\s|[.,!?])`, 'i');
      if (regex.test(lowerTranscript)) {
        return true;
      }
    }

    return false;
  }

  /**
   * Send final response before ending call
   */
  private async sendFinalResponse(
    client: WebSocketClient,
    message: string,
    session: VoiceSession
  ): Promise<void> {
    try {
      logger.info('üé§ SENDING FINAL MESSAGE (v13)', { message });

      let audioDurationMs = 0;

      // Use streaming TTS if available (Deepgram)
      if (session.config.voiceProvider === 'deepgram') {
        audioDurationMs = await this.streamTTSToExotel(client, message, session);
      } else {
        // Fallback to non-streaming for OpenAI/ElevenLabs/Cartesia
        const audioBuffer = await voicePipelineService.synthesizeText(
          message,
          session.config
        );
        await this.sendAudioToExotel(client, audioBuffer, session.streamSid);
        audioDurationMs = (audioBuffer.length / 16000) * 1000;
      }

      // Wait for final message to play before ending call (no "clear" needed)
      const waitTimeMs = Math.max(2000, audioDurationMs * 0.8); // Wait 80% of audio duration or 2s minimum

      logger.info('‚è≥ WAITING FOR FINAL MESSAGE (v13)', {
        audioDurationMs: Math.round(audioDurationMs),
        waitTimeMs: Math.round(waitTimeMs)
      });
      await new Promise(resolve => setTimeout(resolve, waitTimeMs));

      logger.info('‚úÖ FINAL MESSAGE COMPLETE (v13)');
    } catch (error: any) {
      logger.error('Failed to send final response', {
        error: error.message
      });
    }
  }

  /**
   * Clean up session on disconnect
   */
  async handleDisconnect(client: WebSocketClient): Promise<void> {
    const session = this.sessions.get(client.id);
    if (!session) {
      return;
    }

    logger.info('üîå DISCONNECTED (v4)', { processing: session.isProcessing });

    // Clear silence timeout
    if (session.silenceTimeout) {
      clearTimeout(session.silenceTimeout);
    }

    // Close Deepgram streaming connection
    if (session.deepgramConnection) {
      try {
        logger.info('Closing Deepgram streaming connection');
        session.deepgramConnection.finish();
        session.deepgramConnection = undefined;
        logger.info('‚úÖ Deepgram connection closed');
      } catch (error: any) {
        logger.error('Failed to close Deepgram connection', {
          error: error.message
        });
      }
    }

    // Update call log
    await CallLog.findByIdAndUpdate(session.callLogId, {
      $set: {
        status: 'completed',
        endedAt: new Date()
      }
    });

    // Delay cleanup if still processing
    if (session.isProcessing) {
      logger.info('‚è≥ DELAY CLEANUP (v4) - 30s');
      setTimeout(() => {
        this.sessions.delete(client.id);
      }, 30000);
    } else {
      logger.info('üóëÔ∏è DELETE SESSION (v4)');
      this.sessions.delete(client.id);
    }
  }
}

export const exotelVoiceHandler = new ExotelVoiceHandler();
